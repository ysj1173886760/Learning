# coding=utf-8
from __future__ import print_function
import functools
import vgg, pdb, time
import tensorflow as tf, numpy as np, os
import transform
from utils import get_img

STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')
CONTENT_LAYER = 'relu4_2'
DEVICES = '/cpu:0'  #'CUDA_VISIBLE_DEVICES'

os.putenv('MLU_VISIBLE_DEVICES','')

def loss_function(net, content_features, style_features, content_weight, style_weight, tv_weight, preds, batch_size):
    # æŸå¤±å‡½æ•°æ„å»ºï¼Œnet ä¸ºç‰¹å¾æå–ç½‘ç»œï¼Œcontent_features ä¸ºå†…å®¹å›¾åƒç‰¹å¾ï¼Œstyle_features ä¸ºé£æ ¼å›¾åƒç‰¹å¾ï¼Œcontent_weightã€
    # style_weight å’Œ tv_weight åˆ†åˆ«ä¸ºç‰¹å¾é‡å»ºæŸå¤±ã€é£æ ¼é‡å»ºæŸå¤±çš„æƒé‡å’Œå…¨å˜åˆ†æ­£åˆ™åŒ–æŸå¤±çš„æƒé‡

    batch_shape = (batch_size,256,256,3)

    # è®¡ç®—å†…å®¹æŸå¤±
    # content_loss
    content_size = _tensor_size(content_features[CONTENT_LAYER])*batch_size
    assert _tensor_size(content_features[CONTENT_LAYER]) == _tensor_size(net[CONTENT_LAYER])
    content_loss = 2 * content_weight * tf.nn.l2_loss(net[CONTENT_LAYER] - content_features[CONTENT_LAYER]) / content_size

    # è®¡ç®—é£æ ¼æŸå¤±
    # style_loss
    style_losses = []
    for style_layer in STYLE_LAYERS:
        layer = net[style_layer]
        bs, height, width, filters = map(lambda i:i.value,layer.get_shape())
        size = height * width * filters
        feats = tf.reshape(layer, (bs, height * width, filters))
        feats_T = tf.transpose(feats, perm=[0,2,1])
        grams = tf.matmul(feats_T, feats) / size
        style_gram = style_features[style_layer]
        # TODO: è®¡ç®— style_losses
        style_losses.append(2 * tf.nn.l2_loss(grams - style_gram) / size)
    style_loss = style_weight * functools.reduce(tf.add, style_losses) / batch_size

    # ä½¿ç”¨å…¨å˜åˆ†æ­£åˆ™åŒ–æ–¹æ³•å®šä¹‰æŸå¤±å‡½æ•° tv_loss
    # tv_loss
    tv_y_size = _tensor_size(preds[:,1:,:,:])
    tv_x_size = _tensor_size(preds[:,:,1:,:])
    # TODOï¼šå°†å›¾åƒ preds å‘æ°´å¹³å’Œå‚ç›´æ–¹å‘å„å¹³ç§»ä¸€ä¸ªåƒç´ ï¼Œåˆ†åˆ«ä¸åŸå›¾ç›¸å‡ï¼Œåˆ†åˆ«è®¡ç®—äºŒè€…çš„ ğ¿2 èŒƒæ•° x_tv å’Œ y_tv
    # Hint: use tf.nn.l2_loss
    y_tv = tf.nn.l2_loss(preds[:, 1:, :, :] - preds[:, :preds.shape[1] - 1, :, :])
    x_tv = tf.nn.l2_loss(preds[:, :, 1:, :] - preds[:, :, :preds.shape[2] - 1, :])
    tv_loss = tv_weight*2*(x_tv/tv_x_size + y_tv/tv_y_size)/batch_size

    loss = content_loss + style_loss + tv_loss
    return content_loss, style_loss, tv_loss, loss

     
     
#np arr, np arr
def optimize(content_targets, style_target, content_weight, style_weight,
                 tv_weight, vgg_path, epochs=2, print_iterations=1000,
                 batch_size=4, save_path='saver/fns.ckpt', slow=False,
                 learning_rate=1e-3, debug=False, type=0, save=True, load=True):
    # å®æ—¶é£æ ¼è¿ç§»è®­ç»ƒæ–¹æ³•å®šä¹‰ï¼Œcontent_targets ä¸ºå†…å®¹å›¾åƒ, style_target ä¸ºé£æ ¼å›¾åƒ, content_weightã€style_weight å’Œ tv_weight åˆ†åˆ«ä¸º
    # ç‰¹å¾é‡å»ºæŸå¤±ã€é£æ ¼é‡å»ºæŸå¤±å’Œå…¨å˜åˆ†æ­£åˆ™åŒ–é¡¹çš„æƒé‡ï¼Œvgg_path ä¸ºä¿å­˜ VGG19 ç½‘ç»œå‚æ•°çš„æ–‡ä»¶è·¯å¾„
    if slow:
        batch_size = 1
    mod = len(content_targets) % batch_size
    if mod > 0:
        print("Train set has been trimmed slightly..")
        content_targets = content_targets[:-mod] 
    
    # é£æ ¼ç‰¹å¾é¢„å¤„ç†
    style_features = {}

    batch_shape = (batch_size,256,256,3)
    style_shape = (1,) + style_target.shape
    print(style_shape)

    # precompute style features
    with tf.Graph().as_default(), tf.device('/cpu:0'), tf.Session() as sess:
        # ä½¿ç”¨ numpy åº“åœ¨ CPU ä¸Šå¤„ç†
        # TODOï¼šä½¿ç”¨å ä½ç¬¦æ¥å®šä¹‰é£æ ¼å›¾åƒ style_image
        style_image = tf.placeholder(tf.float32, style_shape)

        #TODO: ä¾æ¬¡è°ƒç”¨ vgg.py æ–‡ä»¶ä¸­çš„ preprocess()ã€net() å‡½æ•°å¯¹é£æ ¼å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶å°†æ­¤æ—¶å¾—åˆ°çš„ç‰¹å¾æå–ç½‘ç»œä¼ é€’ç»™ net
        net = vgg.net(vgg_path, vgg.preprocess(style_image))

        # ä½¿ç”¨ numpy åº“å¯¹é£æ ¼å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œå®šä¹‰é£æ ¼å›¾åƒçš„æ ¼æ‹‰å§†çŸ©é˜µ
        style_pre = np.array([style_target])
        for layer in STYLE_LAYERS:
            features = net[layer].eval(feed_dict={style_image:style_pre})
            features = np.reshape(features, (-1, features.shape[3]))
            gram = np.matmul(features.T, features) / features.size
            style_features[layer] = gram

        #TODOï¼šå…ˆä½¿ç”¨å ä½ç¬¦æ¥å®šä¹‰å†…å®¹å›¾åƒ X_contentï¼Œå†è°ƒç”¨ preprocess() å‡½æ•°å¯¹ X_content è¿›è¡Œé¢„å¤„ç†ï¼Œç”Ÿæˆ X_pre
        X_content = tf.placeholder(tf.float32, batch_shape)
        X_pre = vgg.preprocess(X_content)

        # æå–å†…å®¹ç‰¹å¾å¯¹åº”çš„ç½‘ç»œå±‚
        # precompute content features
        content_features = {}
        content_net = vgg.net(vgg_path, X_pre)
        content_features[CONTENT_LAYER] = content_net[CONTENT_LAYER]

        if slow:
            preds = tf.Variable(
                tf.random_normal(X_content.get_shape()) * 0.256
            )
            preds_pre = preds
        else:
            # TODO: å†…å®¹å›¾åƒç»è¿‡å›¾åƒè½¬æ¢ç½‘ç»œåè¾“å‡ºç»“æœ predsï¼Œå¹¶è°ƒç”¨ preprocess() å‡½æ•°å¯¹ preds è¿›è¡Œé¢„å¤„ç†, ç”Ÿæˆ preds_pre
            preds = transform.net(X_content / 255., type)
            preds_pre = vgg.preprocess(preds)

        # TODOï¼špreds_pre è¾“å…¥åˆ°ç‰¹å¾æå–ç½‘ç»œï¼Œå¹¶å°†æ­¤æ—¶å¾—åˆ°çš„ç‰¹å¾æå–ç½‘ç»œä¼ é€’ç»™ net
        net = vgg.net(vgg_path, preds_pre)

        # TODOï¼šè®¡ç®—å†…å®¹æŸå¤± content_loss, é£æ ¼æŸå¤± style_loss, å…¨å˜åˆ†æ­£åˆ™åŒ–é¡¹ tv_loss, æŸå¤±å‡½æ•° loss
        content_loss, style_loss, tv_loss, loss = loss_function(net, content_features, style_features, \
                                                                content_weight, style_weight, tv_weight, \
                                                                preds_pre, batch_size)

        # TODOï¼šåˆ›å»º Adam ä¼˜åŒ–å™¨ï¼Œå¹¶å®šä¹‰æ¨¡å‹è®­ç»ƒæ–¹æ³•ä¸ºæœ€å°åŒ–æŸå¤±å‡½æ•°æ–¹æ³•ï¼Œè¿”å› train_step
        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)

        # TODOï¼šåˆå§‹åŒ–æ‰€æœ‰å˜é‡
        checkpoint_dir = './ckp_temp/fns.ckpt'
        if load:
            print('loading checkpoint')
            saver = tf.train.Saver()
            if os.path.isdir(checkpoint_dir):
                ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
                if ckpt and ckpt.model_checkpoint_path:
                    saver.restore(sess, ckpt.model_checkpoint_path)
                else:
                    raise Exception("No checkpoint found...")
            else:
                saver.restore(sess, checkpoint_dir)
        else:
            sess.run(tf.global_variables_initializer())

        import random
        uid = random.randint(1, 100)
        print("UID: %s" % uid)
        for epoch in range(epochs):
            num_examples = len(content_targets)
            iterations = 0
            while iterations * batch_size < num_examples:
                start_time = time.time()
                curr = iterations * batch_size
                step = curr + batch_size
                X_batch = np.zeros(batch_shape, dtype=np.float32)
                for j, img_p in enumerate(content_targets[curr:step]):
                    X_batch[j] = get_img(img_p, (256,256,3)).astype(np.float32)

                iterations += 1
                assert X_batch.shape[0] == batch_size

                feed_dict = {
                    X_content:X_batch
                }

                train_step.run(feed_dict=feed_dict)
                end_time = time.time()
                delta_time = end_time - start_time
                if debug:
                    print("UID: %s, batch time: %s" % (uid, delta_time))
                print('iteration: %d'%iterations)
                is_print_iter = int(iterations) % print_iterations == 0
                if slow:
                    is_print_iter = epoch % print_iterations == 0
                is_last = epoch == epochs - 1 and iterations * batch_size >= num_examples
                should_print = is_print_iter
                if (iterations == 1 and epoch == 0):
                    to_get = [style_loss, content_loss, tv_loss, loss, preds]
                    test_feed_dict = {
                        X_content:X_batch
                    }

                    tup = sess.run(to_get, feed_dict = test_feed_dict)
                    _style_loss,_content_loss,_tv_loss,_loss,_preds = tup
                    print('Epoch %d, Iteration: %d, Loss: %s' % (epoch, iterations, _loss))
                    to_print = (_style_loss, _content_loss, _tv_loss)
                    print('style: %s, content:%s, tv: %s' % to_print)

                if should_print:
                    to_get = [style_loss, content_loss, tv_loss, loss, preds]
                    test_feed_dict = {
                        X_content:X_batch
                    }

                    tup = sess.run(to_get, feed_dict = test_feed_dict)
                    _style_loss,_content_loss,_tv_loss,_loss,_preds = tup
                    losses = (_style_loss, _content_loss, _tv_loss,_loss)
                    
                    if slow:
                        _preds = vgg.unprocess(_preds)
                    elif save:
                        # TODOï¼šå°†æ¨¡å‹å‚æ•°ä¿å­˜åˆ° save_pathï¼Œå¹¶å°†è®­ç»ƒçš„æ¬¡æ•° save_id ä½œä¸ºåç¼€åŠ å…¥åˆ°æ¨¡å‹åå­—ä¸­
                        saver = tf.train.Saver()
                        res = saver.save(sess, save_path)
                    # å°†ç›¸å…³è®¡ç®—ç»“æœè¿”å›
                    yield(_preds, losses, iterations, epoch)

def _tensor_size(tensor):
    # å¯¹å¼ é‡è¿›è¡Œåˆ‡ç‰‡æ“ä½œï¼Œå°† NHWC æ ¼å¼çš„å¼ é‡ï¼Œåˆ‡ç‰‡æˆ HWCï¼Œå†è®¡ç®— Hã€Wã€C çš„ä¹˜ç§¯
    # å…¶å®å°±æ˜¯è¿”å› H * W * Cï¼Œ åˆ©ç”¨mulè¿›è¡Œreduce
    from operator import mul
    return functools.reduce(mul, (d.value for d in tensor.get_shape()[1:]), 1)
